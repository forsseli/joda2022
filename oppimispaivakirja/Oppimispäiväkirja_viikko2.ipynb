{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb50858",
   "metadata": {},
   "source": [
    "# Viikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92caa1ee",
   "metadata": {},
   "source": [
    "## Osallistuminen opetukseen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef3efb",
   "metadata": {},
   "source": [
    "Tällä viikolla en päässyt osallistumaan opetukseen, sillä minun tuli olla töissä luennon aikana. Katsoin kuitenkin luennon sekä koodiklinikan jälkikäteen panoptosta. Selailin myös läpi tällä viikolla olleen esilukemiston eli Berinaton (2019) artikkelin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2316a",
   "metadata": {},
   "source": [
    "## Luentoviikon keskeiset asiat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832ca81",
   "metadata": {},
   "source": [
    "Tällä viikolla aiheina olivat muun muassa datatiedeprosessin vaiheet, analytiikan nelikenttä, ETL vs. DAD, asiat jotka tiedämme/emme tiedä tietävämme, ryömijät ja raapijat, API-ohjelmointi, dataformaatit ja niiden ohjelmallinen käsittely, data wrangling sekä datan ensitarkastelu Pythonilla. Omasta mielestä kaikista kiinnostavimpia aiheita olivat datatiedeprosessin vaiheet, ETL vs. DAD sekä ryömijät ja raapijat, joten tulen käsittelemään niitä seuraavaksi. Myös analytiikan nelikenttä on mielestäni erittäin mielenkiintoinen, mutta olen tutustunut siihen jo niin paljon esimerkiksi kandidaatintyötäni tehdessä, etten paneudu siihen tällä kertaa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade64f8",
   "metadata": {},
   "source": [
    "Datatieteentyöprosessi koostuu luennolla esitetyn kuvan mukaan neljästä vaiheesta: tiedon esikäsittely, analyysi, reflektio ja tulosten viestiminen. Luennolla painotettiin monta kertaa, miten datan jalostukseen tässä prosessissa kuluu jopa 80% ajasta. Muiden kurssien yhteydessä olen törmännyt saman kaltaisiin prosenttiosuuksiin. Yleensä lähteestä riippuen jalostukseen käytetään noin 60%-90% ajasta. Tämän on todella suuri luku. Itse en ole juurikaan pythonin avulla tehnyt data-analytiikkaa, mutta olen huomannut Power BI:tä käyttäessäni, että datan ollessa hyvin siivottua, on visualisointien tekeminen todella paljon helpompaa. Näin ollen on helppo uskoa, että sama pätee myös koodia kirjoittaessa. Mielestäni tämä myös saattaa selittää Berinaton (2019) mainitseman kuilun tuotetun analytiikan ja sen tulkinnan välillä. Opinnoissani olen törmännyt moniin vastaaviin kuiluihin ja usein ratkaisu on paremmassa viestinnässä. Kun 80% ajasta menee datan jalostukseen, tarvitaan siihen osaavat kaverit. Nämä kaverit eivät ole kuitenkaan välttämättä parhaita viestimään lopputuloksista. Berinato (2019) mainitseekin, että organisaatioon tulisi palkata sellaisia työntekijöitä, joilta löytyy osaamista kaikista niistä osa-alueista, mitä menestyksekkääseen liiketoimintaan tarvitaan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f7973",
   "metadata": {},
   "source": [
    "Luennolla ei käsitelty käsitteitä ETL ja DAD kovinkaan laajasti, mutta itsellä heräsi niihin mielenkiinto. ETL on tullut vastaan jo monesti tietojohtamisen kursseilla, mutta DAD oli itselle uusi tuttavuus. Luennolla oli yksi linkki, josta näitä asioita olisi voinut tutkia, mutta ainakaan itsellä se ei toiminut. Tutustuin kuitenkin näihin asioihin useiden lähteiden avulla ja lopulta loin itselleni seuraavan käsityksen käsitteistä. ETL (Extract/Load/Transform) on erityisesti data engineerien käytössä ja sen avulla pyritään varmistamaan datan sulava liikkuminen lähteen ja kohteen välillä. DAD (Discover/Access /Distill) on puolestaan erityisesti data scientistien hyödyntämä malli, jota hyödynnetään datan prosessoinnissa. Luennolle sanottiinkin, että ETL on huomattavasti pidempi prosessi kuin DAD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43febb8",
   "metadata": {},
   "source": [
    "Ryömijät ja raapijat olivat itselle uusia asioita, enkä tiennyt niistä mitään ennen tätä kurssia. Ryömijöiden tarkoituksena on käydä verkko-osoitteita läpi ja indeksoida niitä. Raapijoiden tarkoitus taas on kerätä tietoa verkkosivulta esimerkiksi tunnistamalla metaelementtejä. Näitä yhdistelemällä saadaan siis paljon tietoa kerättyä. Luennolla käytiin aika paljon läpi näiden käytön laillisuutta. Luentomateriaalissa olikin linkki artikkeliin, jossa lueteltiin asioita, joidenka takia ryömijöiden ja raapijoiden käyttö nähdään usein negatiivisesti. Niitä käytetään esimerkiksi taloudellisen edun saamiseksi, laeista välittämättä ja loukkaavasti. Itselle jäikin sellainen kuva, että ne ovat loistavia työkaluja oikein käytettyinä."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10115cb",
   "metadata": {},
   "source": [
    "## Viisi oivallusta / tärkeintä vinkkiä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b7707",
   "metadata": {},
   "source": [
    "* Datan jalostukseen menee jopa 80% ajasta\n",
    "* Raapijat ja ryömijät hyviä työkaluja, mutta niiden käytön kanssa tulee olla tarkkana\n",
    "* Datatieteilijälle hyödyllisempää osata paljon kaikkea, kuin todella hyvin yhtä asiaa\n",
    "* Datatyyppien kanssa tulee olla erittäin tarkkana koodatessa\n",
    "* Datatieteen prosessi on monimutkainen ja harva on hyvä prosessin jokaisessa vaiheessa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8226de",
   "metadata": {},
   "source": [
    "## Kehityskohteita / flippausvinkkejä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3dbb7",
   "metadata": {},
   "source": [
    "* Luennolla mainittiin, että kartoittavaan analytiikkaan palataan myöhemmin. Luentomateriaalissa olisi voinut olla kuitenkin linkki johonkin aihetta käsittelevään artikkeliin, jotta asiaan olisi voinut perehtyä halutessaan jo aikaisemmin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7df60a",
   "metadata": {},
   "source": [
    "## Demo viikon aikana oppimastani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e04e4",
   "metadata": {},
   "source": [
    "Tällä viikolla käsiteltiin koodia luennolla sekä koodiklinikassa. Luennolla jalostettiin dataa ja koodiklinikassa hyödynnettiin raapijaa. Esittelen ensin muutamalla rivillä luennolla oppimaani koodia ja tämän jälkeen koodiklinikassa oppimaani."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35becfb3",
   "metadata": {},
   "source": [
    "Kuvitellaan, että meillä on dataframe nimeltään \"test\". Luennolla opetettiin, miten saadaan varmistettua, että kaikki ID-arvot ovat uniikkeja ja miten ne voidaan vaihtaa indekseiksi. Kuvitellaan, että dataframessa on sarake ID, jolloin voimme tehdä seuraavasti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ID'].is_unique\n",
    "test.set_index('ID', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ede9a",
   "metadata": {},
   "source": [
    "Voimme myös selvittää kaikki dataframen datatyypit koodilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8c315",
   "metadata": {},
   "source": [
    "Tämän perusteella voidaan muuttaa haluttujen sarakkeiden datatyyppejä. Yksi luennolla paljon esillä ollut komento oli myös groupby, jonka avulla dataa voi ryhmitellä. Esimerkiksi jos data halutaan esittää ryhmittelemällä sarakkeen \"name\" avulla ja halutaan saada selville keskiarvo, voidaan kirjoittaa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f460646",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby('name').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a34ba",
   "metadata": {},
   "source": [
    "Seuraavaksi siirrytään koodiklinikan pariin. Ensimmäisenä luodaan python-tiedosto, johon muodostuu runko raapijalle. Voidaan esimerkiksi miettiä, että tässä esimerkissä dataa raavittaisiin gigantin sivuilta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy genspider gigantti_scraper gigantti.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a0131",
   "metadata": {},
   "source": [
    "Jotta raapija osaa etsiä arvostelut sivulta, tulee osata etsiä Inspect-toiminnon avulla, miten arvostelu on sijoitettu nettisivun rakenteeseen. Kun tämä on selvitetty, voidaan etsiä arvostelut ja tallentaa ne tiedostoon. Jotta teksti tallettuisi yksinkertaisessa muodossa voidaan kirjoittaa esimerkiksi seuraavalla tavalla (esimerkki lähes suoraan koodiklinikasta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = response.css('.a-size-base.review-text')\n",
    "\n",
    "        for i in range(len(review_texts)):\n",
    "            reviews[i] = \"\".join(review_texts[i].css('::text').extract()).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47751c7",
   "metadata": {},
   "source": [
    "Edellisessä koodissa etsittiin ensin oikeat kohdat nettisivusta. Tämän jälkeen nämä kohdat käytiin läpi ja extract-komennon avulla kerättiin pelkästään tekstiosuus. Join-komennon avulla saadaan yhdistettyä kaikki osat yhdeksi ja strip-komennon avulla saadaan poistettua kaikki turha arvostelun ympäriltä."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c6886",
   "metadata": {},
   "source": [
    "Vielä viimeiseksi voidaa ajaa edellä luoto python-tiedosto ja tallettaa tulokset json-tiedostoon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca17731",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy runspider gigantti_scraper.py -o gigantti.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190674a",
   "metadata": {},
   "source": [
    "## Lähteet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3963be",
   "metadata": {},
   "source": [
    "Berinato, S. (2019). Data Science and the Art of Persuasion. Saatavilla www-muodossa: https://hbr.org/2019/01/data-science-and-the-art-of-persuasion?utm_campaign=hbr&utm_medium=social&utm_source=twitter (Luettu 17.03.2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
